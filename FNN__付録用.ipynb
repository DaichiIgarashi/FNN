{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaichiIgarashi/FNN/blob/main/FNN__%E4%BB%98%E9%8C%B2%E7%94%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foMZosEGAjTx"
      },
      "source": [
        "# 1) セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKO0LGnV4_ni"
      },
      "outputs": [],
      "source": [
        "#Google Colabのセットアップ\n",
        "!pip install PyDrive\n",
        "!pip install -U keras \n",
        "!pip install tensorflow==2.4.1\n",
        "!pip install keras==2.4.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6R9n_vk5jUT"
      },
      "outputs": [],
      "source": [
        "#ライブラリをインポート\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from oauth2client.client import GoogleCredentials\n",
        "#from google.colab.patches import cv2_imshow\n",
        "\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import LeakyReLU, Dense\n",
        "#from keras.preprocessing import image\n",
        "\n",
        "#from PIL import Image\n",
        "#from skimage.io import imread\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "from tqdm import tqdm\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiPoVhnzGzdk"
      },
      "source": [
        "# 2) 認証\n",
        "(パスワード入力)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjbedmiZ51XE"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "Drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2fBJF_k1hqT"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No7X0S1hArug"
      },
      "source": [
        "# 3) データをダウンロード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en54WS5zvcCu"
      },
      "source": [
        "1 Input画像はGoogle Driveへdata.zipとしてダウンロード\\\n",
        "2 Outputはanswer.csvとしてダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDrx4ve1mIiv"
      },
      "outputs": [],
      "source": [
        "#train動画のリンクの貼り付け\n",
        "#https://drive.google.com/file/d/1f1XWtM0CtO7UvpHNl4Aw6MZ0sNi3M9qb/view?usp=sharing\n",
        "\n",
        "download = Drive.CreateFile({'id': '1f1XWtM0CtO7UvpHNl4Aw6MZ0sNi3M9qb'}) #リンクのIDを貼り付け\n",
        "download.GetContentFile('data.zip') #フォルダの名前\n",
        "!unzip data.zip #フォルダの名前(zip解除)\n",
        "\n",
        "#dataを置き換えるか聞いてくる\n",
        "#[y]は一つずつ順に置き換えるか聞いてくる\n",
        "#[n]は置き換えない\n",
        "#[A]は全て置き換える\n",
        "#[N]は最初の一つだけ置き換える\n",
        "#[r]は名前の変更"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP5piGxYmP8X"
      },
      "outputs": [],
      "source": [
        "#Outputのリンクの貼り付け\n",
        "# https://drive.google.com/file/d/1dyqDt5SEXgZF20FQOgtIQ3H5qS0ZkQI3/view?usp=sharing\n",
        "\n",
        "download = Drive.CreateFile({'id': '1dyqDt5SEXgZF20FQOgtIQ3H5qS0ZkQI3'})\n",
        "download.GetContentFile('data.csv')\n",
        "\n",
        "# download = Drive.CreateFile({'id': '1xVLnttfdqvkyZH2M7ydtNQqQJmgd9h7j'})\n",
        "# download.GetContentFile('test.csv')\n",
        "\n",
        "M_answer = pd.read_csv('data.csv')\n",
        "M_id = M_answer['id'].values\n",
        "M_label = M_answer['label'].values\n",
        "\n",
        "# test_answer = pd.read_csv('test.csv')\n",
        "# test_id = test_answer['id'].values\n",
        "# test_label = test_answer['label'].values\n",
        "\n",
        "n_labels = len(np.unique(M_label))\n",
        "M_label = np.eye(n_labels)[M_label-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIqnb0XZ_A07"
      },
      "outputs": [],
      "source": [
        "#画像を出力\n",
        "data_image = []\n",
        "view_data_image = []\n",
        "for i in tqdm(range(M_id.shape[0])):\n",
        "    image = cv2.imread('data/'+'data'+M_id[i].astype('str')+'.tif',0)\n",
        "    print(image.shape)\n",
        "    img = image/255\n",
        "    #img = 1-img\n",
        "    img = (img-np.min(img))/(np.max(img)-np.min(img))\n",
        "    view_data_image.append(img)\n",
        "    img = img.reshape((image.shape[0]*image.shape[1]))\n",
        "    data_image.append(img)\n",
        "    \n",
        "    plt.imshow(view_data_image[i].squeeze(), cmap=plt.cm.gray)\n",
        "    # plt.show()\n",
        "    print('Label = ' + str(i+1))\n",
        "\n",
        "img_h = image.shape[0]\n",
        "img_w = image.shape[1]\n",
        "M_imgs = np.array(data_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0r7DG_UmxeO"
      },
      "outputs": [],
      "source": [
        "#M_allに各要素を追加\n",
        "M_all = []\n",
        "for i in range(M_id.shape[0]):\n",
        "  M_all_append = [M_id[i],M_label[i],M_imgs[i]]\n",
        "  M_all.append(M_all_append)\n",
        "\n",
        "# print(M_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2urMbn5tGqW"
      },
      "outputs": [],
      "source": [
        "def comb(k):\n",
        "  train_data = []\n",
        "  test_data = []      \n",
        "  for i in range(M_id.shape[0]):\n",
        "    if k*M_id.shape[0]/10 <= i < (k+1) *M_id.shape[0]/10:\n",
        "      test_data_append = [M_id[i],M_label[i],M_imgs[i]]\n",
        "      test_data.append(test_data_append)\n",
        "    elif (k+5)*M_id.shape[0]/10 <= i < (k+6) *M_id.shape[0]/10:\n",
        "      test_data_append = [M_id[i],M_label[i],M_imgs[i]]\n",
        "      test_data.append(test_data_append)\n",
        "    else:\n",
        "      train_data_append = [M_id[i],M_label[i],M_imgs[i]]\n",
        "      train_data.append(train_data_append)\n",
        "\n",
        "  return train_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DexlTj3inYy"
      },
      "outputs": [],
      "source": [
        "#Preparing data for each combination\n",
        "id_train = []\n",
        "id_test = []\n",
        "y_train = []\n",
        "y_test = []\n",
        "x_train = []\n",
        "x_test = []\n",
        "\n",
        "for i in range(5):\n",
        "  id_train.append(np.array([el[0] for el in comb(i)[0]]))\n",
        "  id_test.append(np.array([el[0] for el in comb(i)[1]]))\n",
        "  # y_train.append(to_categorical([el[1] for el in comb(i)[0]]))\n",
        "  # y_test.append(to_categorical([el[1] for el in comb(i)[1]]))\n",
        "  y_train.append(np.array([el[1] for el in comb(i)[0]]))\n",
        "  y_test.append(np.array([el[1] for el in comb(i)[1]]))\n",
        "  x_train.append(np.array([el[2] for el in comb(i)[0]]))\n",
        "  x_test.append(np.array([el[2] for el in comb(i)[1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf6_8LFK6fHs"
      },
      "source": [
        "# 4) ニューラルネットワークの構造"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "473TJEf-TTPx"
      },
      "outputs": [],
      "source": [
        "#モデルの構造を定義\n",
        "model = Sequential()\n",
        "  \n",
        "#隠れ層\n",
        "#Denceはニューロンの数，増やしすぎると分散しちゃう，出力層に向かってニューロンを減らしていくと良い\n",
        "# model.add(Dense(5, activation = 'relu', input_shape = (img_h*img_w,))) #1層目\n",
        "# model.add(Dense(10, activation = 'relu')) #2層目\n",
        "# model.add(Dense(5, activation = 'relu')) #3層目\n",
        "\n",
        "#LeakyReLUを使う場合\n",
        "leakyalpha = 0.01\n",
        "model.add(Dense(1, input_shape = (img_h*img_w,)))\n",
        "model.add(tensorflow.keras.layers.LeakyReLU(alpha = leakyalpha))\n",
        "# model.add(tensorflow.keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None))\n",
        "# model.add(tensorflow.keras.layers.ELU(alpha=1.0))\n",
        "\n",
        "#出力層\n",
        "#model.add(Dense(1)) #回帰\n",
        "model.add(Dense(2, activation = 'softmax', input_shape = (img_h*img_w,))) #分類 2クラス分類は'sigmoid'，多クラス分類は'softmax'\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-flnewYFuibx"
      },
      "outputs": [],
      "source": [
        "#モデルを保存\n",
        "date = 'softmax' #保存場所\n",
        "\n",
        "path_result = '/content/drive/My Drive/result_'+ date +'/'\n",
        "if os.path.exists(path_result) == 0:\n",
        "  os.mkdir(path_result)\n",
        "\n",
        "path_model = path_result + 'model/'\n",
        "if os.path.exists(path_model) == 0:\n",
        "  os.mkdir(path_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6J-_jU7vkDe"
      },
      "outputs": [],
      "source": [
        "#ハイパーパラメータの設定\n",
        "epoch = 10000 #学習の回数\n",
        "batch = 32 #バッチ数\n",
        "split = 0.2 #validation率\n",
        "LR = 0.00005 #学習率\n",
        "ptnc = 500 #early stoppingを何回目でやるか\n",
        "\n",
        "#callback(early stopping)の定義\n",
        "callback = tensorflow.keras.callbacks.EarlyStopping(\n",
        "                     monitor='val_loss', min_delta=0.000,\n",
        "                     patience=ptnc, restore_best_weights=True)\n",
        "\n",
        "\n",
        "#Optimizerの定義\n",
        "#sgd\n",
        "opt = optimizers.SGD(learning_rate = LR, momentum = 0.9) #(推奨: LR=0.0005,Mmntm=0.9)\n",
        "#adam\n",
        "#opt = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) #強い (推奨: LR=0.01)\n",
        "#adagrad\n",
        "#opt = optimizers.Adagrad(lr=LR, epsilon=None, decay=0.0) #(推奨: LR=0.0001)\n",
        "#adadelta\n",
        "#opt = optimizers.Adadelta(lr=LR, rho=0.95, epsilon=None, decay=0.0 #(推奨: LR=0.0001)\n",
        "#adamax\n",
        "#opt = optimizers.Adamax(lr=LR, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0) #(推奨: LR=0.0001)\n",
        "#nadam\n",
        "#opt = optimizers.Nadam(lr=LR, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004) #(推奨: LR=0.0001)\n",
        "#rmsprop\n",
        "#opt = optimizers.RMSprop(lr=LR, rho=0.9, epsilon=None, decay=0.0) #RNNで有用性あり\n",
        "\n",
        "#https://keras.io/ja/optimizers/ 参照"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOA3GBXuChCG"
      },
      "outputs": [],
      "source": [
        "#modelをコンパイル\n",
        "#保存したモデルをロード\n",
        "# if os.path.exists(path_model+'my_model.h5') == 1:\n",
        "#   model = load_model(path_model+'my_model.h5')\n",
        "\n",
        "# model.compile(optimizer=opt,loss='mse') #optimizer変更 回帰\n",
        "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics='acc') #分類\n",
        "model.save(path_model+'my_model.h5') #モデルを保存"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fmT8TbFCk5V"
      },
      "outputs": [],
      "source": [
        "#重みとバイアスの初期設定\n",
        "weight1_initial = model.layers[0].get_weights()[0]\n",
        "bias1_initial = model.layers[0].get_weights()[1]\n",
        "# weight2_initial = model.layers[1].get_weights()[0]\n",
        "# bias2_initial = model.layers[1].get_weights()[1]\n",
        "# weight3_initial = model.layers[2].get_weights()[0]\n",
        "# bias3_initial = model.layers[2].get_weights()[1]\n",
        "weight_out_initial = model.layers[len(model.layers)-1].get_weights()[0]\n",
        "bias_out_initial = model.layers[len(model.layers)-1].get_weights()[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JvVNsLaljgX"
      },
      "source": [
        "# 5) 学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyyoAZTzPFNt"
      },
      "outputs": [],
      "source": [
        "#Combination1の学習\n",
        "model1 = load_model(path_model+'my_model.h5') #保存したモデルを出力\n",
        "history1 = model1.fit(x_train[0], y_train[0], batch_size = batch,\n",
        "                      epochs = epoch, verbose = 2, callbacks = callback, #callbecksの前に#を入れればEarlyStoppingなし\n",
        "                      validation_split = split)\n",
        "model1.save(path_model+'my_model1.h5') #モデルの保存\n",
        "#drive/Mydrive/result_???/logからval_lossを見ることが可能"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RZ0G9XAOYMo_"
      },
      "outputs": [],
      "source": [
        "#Combination2の学習\n",
        "model2 = load_model(path_model+'my_model.h5') #保存したモデルを出力\n",
        "history2 = model2.fit(x_train[1], y_train[1], batch_size = batch,\n",
        "                      epochs = epoch, verbose = 2, callbacks = callback, #callbecksの前に#を入れればEarlyStoppingなし\n",
        "                      validation_split = split)\n",
        "model2.save(path_model+'my_model2.h5') #モデルの保存\n",
        "#drive/Mydrive/result_???/logからval_lossを見ることが可能"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OTKIzYb8Yb1-"
      },
      "outputs": [],
      "source": [
        "#Combination3の学習\n",
        "model3 = load_model(path_model+'my_model.h5') #保存したモデルを出力\n",
        "history3 = model3.fit(x_train[2], y_train[2], batch_size = batch,\n",
        "                      epochs = epoch, verbose = 2, callbacks = callback, #callbecksの前に#を入れればEarlyStoppingなし\n",
        "                      validation_split = split)\n",
        "model3.save(path_model+'my_model3.h5') #モデルの保存\n",
        "#drive/Mydrive/result_???/logからval_lossを見ることが可能"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCEdZcHuYeKO"
      },
      "outputs": [],
      "source": [
        "#Combination4の学習\n",
        "model4 = load_model(path_model+'my_model.h5') #保存したモデルを出力\n",
        "history4 = model4.fit(x_train[3], y_train[3], batch_size = batch,\n",
        "                      epochs = epoch, verbose = 2, callbacks = callback, #callbecksの前に#を入れればEarlyStoppingなし\n",
        "                      validation_split = split)\n",
        "model4.save(path_model+'my_model4.h5') #モデルの保存\n",
        "#drive/Mydrive/result_???/logからval_lossを見ることが可能"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fmhGC4u2YgaP"
      },
      "outputs": [],
      "source": [
        "#Combination5の学習\n",
        "model5 = load_model(path_model+'my_model.h5') #保存したモデルを出力\n",
        "history5 = model5.fit(x_train[4], y_train[4], batch_size = batch,\n",
        "                      epochs = epoch, verbose = 2, callbacks = callback, #callbecksの前に#を入れればEarlyStoppingなし\n",
        "                      validation_split = split)\n",
        "model5.save(path_model+'my_model5.h5') #モデルの保存\n",
        "#drive/Mydrive/result_???/logからval_lossを見ることが可能"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O-vAvoKyZsAU"
      },
      "outputs": [],
      "source": [
        "#モデルをロード\n",
        "model1 = load_model(path_model+'my_model1.h5')\n",
        "model2 = load_model(path_model+'my_model2.h5')\n",
        "model3 = load_model(path_model+'my_model3.h5')\n",
        "model4 = load_model(path_model+'my_model4.h5')\n",
        "model5 = load_model(path_model+'my_model5.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H4NBaZZ_9ovT"
      },
      "outputs": [],
      "source": [
        "#学習したモデルをコンパイル\n",
        "model_all = []\n",
        "model_all.append(model1)\n",
        "model_all.append(model2)\n",
        "model_all.append(model3)\n",
        "model_all.append(model4)\n",
        "model_all.append(model5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UUbQZY3OEH4-"
      },
      "outputs": [],
      "source": [
        "#学習履歴を保存\n",
        "path_log = path_result + 'log/'\n",
        "if os.path.exists(path_log) == 0:\n",
        "  os.mkdir(path_log)\n",
        "\n",
        "for i in range(1,6):\n",
        "  k = globals()['history'+str(i)]\n",
        "  log = pd.DataFrame(k.history)\n",
        "  log_name = path_log+'log_comb'+str(i)+'.csv'\n",
        "  log.to_csv(log_name, header=True, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glWAFLJ47jet"
      },
      "source": [
        "# 6) 損失をグラフ化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5FKTqH3Bnv5"
      },
      "source": [
        "## グラフの準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tGPjbFYKBG_t"
      },
      "outputs": [],
      "source": [
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "fs = 16\n",
        "\n",
        "font = FontProperties()\n",
        "font.set_family('Serif')\n",
        "# font.set_name('Times New Roman')\n",
        "font.set_size(fs)\n",
        "\n",
        "font_leg = FontProperties()\n",
        "font_leg.set_family('Serif')\n",
        "# font.set_name('Times New Roman')\n",
        "font_leg.set_size(12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwPoGzpl2-AP"
      },
      "source": [
        "## プロットする関数を定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ondtQuhf0cUv"
      },
      "outputs": [],
      "source": [
        "#相対誤差のプロット\n",
        "# u_mean = 5.35\n",
        "\n",
        "def plot_loss_acc(comb):\n",
        "\n",
        "  # k = globals()['history' + str(comb)]\n",
        "\n",
        "  # train_loss = k.history['loss']\n",
        "  # val_loss = k.history['val_loss']\n",
        "  # train_acc = k.history['acc']\n",
        "  # val_acc = k.history['val_acc']\n",
        "\n",
        "  log_name = path_log+'log_comb'+str(i)+'.csv'\n",
        "  k = pd.read_csv(log_name)\n",
        "\n",
        "  # train_loss = k['loss']\n",
        "  # val_loss = k['val_loss']\n",
        "  train_acc = k['acc']\n",
        "  val_acc = k['val_acc']\n",
        "\n",
        "  train_acc = 100 * train_acc\n",
        "  val_acc = 100 * val_acc\n",
        "\n",
        "  label_train_acc = 'Comb. ' + str(comb) + ' (train.)'\n",
        "  label_val_acc = 'Comb. ' + str(comb) + ' (val.)'\n",
        "  \n",
        "  epoch_plt = list(range(1, len(val_acc) + 1))\n",
        "  \n",
        "  plt.plot(epoch_plt, train_acc, label = label_train_acc)\n",
        "  plt.plot(epoch_plt, val_acc, label = label_val_acc)\n",
        "\n",
        "  # fig = plt.figure()\n",
        "\n",
        "  # train_loss_new = (train_loss**0.5/u_mean)*100 #相対誤差\n",
        "  # val_loss_new = (val_loss**0.5/u_mean)*100\n",
        "\n",
        "  #color1 = 'tab:red'\n",
        "  # ax1 = fig.add_subplot(111)\n",
        "  # lns1 = ax1.plot(epoch_plt, train_acc, '-r', \n",
        "  #                 label = 'Traning accuracy')#, color=color1)\n",
        "  # lns2 = ax1.plot(epoch_plt, val_acc,\n",
        "  #                 label = 'Validation accuracy')#, color='tab:orange')\n",
        "  plt.xticks(fontsize=fs)\n",
        "  plt.yticks(fontsize=fs)\n",
        "\n",
        "  plt.xlabel('Epoch [-]', fontproperties=font)\n",
        "  plt.ylabel('Accuracy [%]', fontproperties=font)\n",
        "  # plt.title('Loss vs. Epoch', fontproperties=font)\n",
        "  plt.legend(loc='lower right', prop=font_leg, fancybox=False, edgecolor=\"black\")\n",
        "  \n",
        "  #color2 = 'tab:blue'\n",
        "  # ax2 = ax1.twinx()\n",
        "  # lns3 = ax2.plot(epoch_plt, train_acc, '-b',\n",
        "  #                 label = 'Training accuracy')#, color=color2)\n",
        "  # lns4 = ax2.plot(epoch_plt, val_acc, '-g',\n",
        "  #                 label = 'Validation accuracy')#, color=color2)\n",
        "  # plt.yticks(fontsize=fs)\n",
        "  \n",
        "  # lns = lns1+lns2#+lns3+lns4\n",
        "  # labs = [l.get_label() for l in lns]\n",
        "  # leg = ax1.legend(lns, labs, loc='lower right', prop=font_leg, fancybox=False, edgecolor=\"black\")\n",
        "\n",
        "  #ax1.grid()\n",
        "  # ax1.set_xlabel('Epoch [-]', fontproperties=font)\n",
        "  # ax1.set_ylabel('Accuracy [%]', fontproperties=font)\n",
        "  #ax2.set_ylabel('Accuracy [%]', fontproperties=font, rotation=-90, labelpad=15)\n",
        "  #ax1.set_xlim(0,8000) #表示範囲の設定\n",
        "  #ax1.set_ylim(-5,105) #表示範囲の設定\n",
        "  #ax2.set_ylim(-5,105)\n",
        "  \n",
        "  # title_loss_acc = 'Combination ' + str(comb)\n",
        "  #plt.title(title_loss_acc, fontproperties=font)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N1vCCQb6wtU7"
      },
      "outputs": [],
      "source": [
        "#損失のプロット\n",
        "def plot_loss(comb):\n",
        "  # k = globals()['history' + str(comb)]\n",
        "  \n",
        "  # train_loss = k.history['loss']\n",
        "  # val_loss = k.history['val_loss']\n",
        "\n",
        "  log_name = path_log+'log_comb'+str(i)+'.csv'\n",
        "  k = pd.read_csv(log_name)\n",
        "\n",
        "  train_loss = k['loss']\n",
        "  val_loss = k['val_loss']\n",
        "  \n",
        "  label_train_loss = 'Comb. ' + str(comb) + ' (train.)'\n",
        "  label_val_loss = 'Comb. ' + str(comb) + ' (val.)'\n",
        "  \n",
        "  epoch_plt = list(range(1, len(val_loss) + 1))\n",
        "  plt.plot(epoch_plt, train_loss, label = label_train_loss)\n",
        "  plt.plot(epoch_plt, val_loss, label = label_val_loss)\n",
        "  #plt.ylim(0.0,1)\n",
        "\n",
        "  plt.xticks(fontsize=fs)\n",
        "  plt.yticks(fontsize=fs)\n",
        "  plt.xlabel('Epoch [-]', fontproperties=font)\n",
        "  plt.ylabel('Error [-]', fontproperties=font)\n",
        "  # plt.title('Loss vs. Epoch', fontproperties=font)\n",
        "  plt.legend(loc='upper right', prop=font_leg, fancybox=False, edgecolor=\"black\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuqH7arF3BRS"
      },
      "source": [
        "## プロット"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lGqVwZE7zEEm"
      },
      "outputs": [],
      "source": [
        "path_LossAcc = path_result + 'LossAcc/'\n",
        "if os.path.exists(path_LossAcc) == 0:\n",
        "  os.mkdir(path_LossAcc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC1O_uNhWB_i"
      },
      "outputs": [],
      "source": [
        "#η-Epochグラフ\n",
        "for i in range(1,6):\n",
        "  plot_loss_acc(i)\n",
        "  plt.savefig(path_LossAcc+'CombALL_Acc.png', dpi=300, bbox_inches='tight')\n",
        "  # plt.show()\n",
        "##drive/Mydrive/result_???/LossAccからグラフを見ることが可能"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CnMooLgVwd37"
      },
      "outputs": [],
      "source": [
        "#MSE-Epochグラフ\n",
        "for i in range(1,6):\n",
        "  plot_loss(i)\n",
        "  plt.savefig(path_LossAcc+'CombALL_Loss.png', dpi=300, bbox_inches='tight')\n",
        "  # plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HxxkG8ahdK-"
      },
      "source": [
        "# 7) テスト\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2rF2jBFdhhXH"
      },
      "outputs": [],
      "source": [
        "#Define function for test result of comb_k\n",
        "#variables: prob, pred, TF, result_test, Correct, Wrong\n",
        "def result_test(comb):\n",
        "  k = comb - 1\n",
        "  \n",
        "  #Checking the prediction with answer\n",
        "  TF = []\n",
        "  for i in range(pred[k].shape[0]):\n",
        "    if pred[k][i] == y_test[k][i,1]:\n",
        "      tfpn = 'T'\n",
        "    else:\n",
        "      tfpn = 'F'\n",
        "    if pred[k][i] == 1:\n",
        "      tfpn = tfpn + 'P'\n",
        "    else:\n",
        "      tfpn = tfpn + 'N'\n",
        "    TF.append(tfpn)\n",
        "    \n",
        "  TF = np.array(TF)\n",
        "  \n",
        "  #Saving test result on a df\n",
        "  # result_test = pd.DataFrame(H_test[k][:], columns = ['H'])\n",
        "  result_test = pd.DataFrame(id_test[k][:], columns = ['id'])\n",
        "  # result_test['id'] = id_test[k][:]\n",
        "  result_test['prob.'] = prob[k][:,1]\n",
        "  result_test['pred.'] = pred[k]\n",
        "  result_test['ans.'] = y_test[k][:,1].astype(int)\n",
        "  result_test['T/F'] = TF\n",
        "  result_test['q_out'] = q_out_test[k][:,1]\n",
        "  result_test['y_pred'] = y_pred_test[k][:,1]\n",
        "\n",
        "  result_test_name = path_test+'test_comb'+str(comb)+'.csv'\n",
        "  result_test.to_csv(result_test_name, header=True, index=False)\n",
        "  \n",
        "  #Counting correct/wrong prediction\n",
        "  TP = np.count_nonzero(TF == 'TP')\n",
        "  TN = np.count_nonzero(TF == 'TN')\n",
        "  Total_P = np.count_nonzero(y_test[k][:,1] == 1)\n",
        "  Total_N = np.count_nonzero(y_test[k][:,1] == 0)\n",
        "  Correct = TP + TN\n",
        "  TP_pc = TP/Total_P*100\n",
        "  TN_pc = TN/Total_N*100\n",
        "  Correct_pc = Correct/TF.shape[0]*100\n",
        "\n",
        "  TPTN.append([comb, TP, Total_P, TP_pc, TN, Total_N, TN_pc, Correct,\n",
        "               TF.shape[0], Correct_pc])\n",
        "  \n",
        "  #Print result\n",
        "  # print('Comb_',comb,':')\n",
        "  # print('True Positive:',TP,'/',Total_P)\n",
        "  # print('True Negative:',TN,'/',Total_N)\n",
        "  # print('Total Correct:',Correct,'/',TF.shape[0])\n",
        "\n",
        "  return result_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AsBScTHKhkGE"
      },
      "outputs": [],
      "source": [
        "path_test = path_result + 'test/'\n",
        "\n",
        "if os.path.exists(path_test) == 0:\n",
        "  os.mkdir(path_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q6L5r37ghl91"
      },
      "outputs": [],
      "source": [
        "#Compiling the probability & prediction of all combinations\n",
        "prob = []\n",
        "pred = []\n",
        "\n",
        "for i in range(1,6):\n",
        "  i = i - 1\n",
        "  # prob.append(model_all[i].predict_proba(x_test[i]))\n",
        "  # pred.append(model_all[i].predict_classes(x_test[i]))\n",
        "  prob.append(model_all[i].predict(x_test[i]))\n",
        "  pred.append(np.argmax(model_all[i].predict(x_test[i]), axis=-1))\n",
        "\n",
        "# for i in range(1,6):\n",
        "#   i = i - 1\n",
        "#   q_out.append(-np.log((1/prob[i])-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0p3F7HWiZMjM"
      },
      "outputs": [],
      "source": [
        "#重みとバイアスの抽出\n",
        "weight1 = []\n",
        "bias1 = []\n",
        "weight_out = []\n",
        "bias_out = []\n",
        "\n",
        "for i in range(5):\n",
        "  weight1.append(model_all[i].layers[0].get_weights()[0])\n",
        "  bias1.append(model_all[i].layers[0].get_weights()[1])\n",
        "  weight_out.append(model_all[i].layers[len(model_all[i].layers)-1].get_weights()[0])\n",
        "  bias_out.append(model_all[i].layers[len(model_all[i].layers)-1].get_weights()[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MZAH5GpRjCyC"
      },
      "outputs": [],
      "source": [
        "W1s_test = []\n",
        "q1_test = []\n",
        "s1_test = []\n",
        "q_out_test = []\n",
        "y_pred_test = []\n",
        "\n",
        "def relu(x):\n",
        "  return x * (x>0)\n",
        "def leakyRELU(x):\n",
        "  return np.where(x > 0, x, x * leakyalpha)\n",
        "def sigomoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "def softmax(x):\n",
        "  if (x.ndim == 1):\n",
        "    x = x[None,:]    # ベクトル形状なら行列形状に変換\n",
        "  # テンソル（x：行列）、軸（axis=1： 列の横方向に計算）\n",
        "  return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "\n",
        "for i in range(5):\n",
        "  W1s_test.append(np.dot(x_test[i],weight1[i]))\n",
        "  q1_test.append(W1s_test[i] + bias1[i])\n",
        "  s1_test.append(leakyRELU(q1_test[i]))\n",
        "  q_out_test.append(np.dot(s1_test[i], weight_out[i]) + bias_out[i])\n",
        "  y_pred_test.append(softmax(q_out_test[i]))\n",
        "  # y_pred = (np.exp(q_out[i]) / (np.sum(np.exp(q_out[i]), axis = 1)).reshape(len(M_all),1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hTBARfiQhn5u"
      },
      "outputs": [],
      "source": [
        "# #Test result for all combinations\n",
        "result_test_all = []\n",
        "TPTN = []#[['Cmb','TP','P','TN','N','Cor','T']]\n",
        "\n",
        "for i in range(1,6):\n",
        "  result_test_all.append(result_test(i))\n",
        "  #print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ScmpqiE8hpWF"
      },
      "outputs": [],
      "source": [
        "TPTN_df = pd.DataFrame(columns=['Comb','TP','Tot P', 'TP%','TN','Tot N', 'TN%',\n",
        "                                'Cor','Tot','Cor%'])\n",
        "\n",
        "for i in range(len(TPTN)):\n",
        "  TPTN_add = pd.DataFrame(\n",
        "      [TPTN[i]],columns=['Comb','TP','Tot P', 'TP%','TN','Tot N', 'TN%','Cor',\n",
        "                         'Tot','Cor%'])\n",
        "  TPTN_df = TPTN_df.append(TPTN_add,ignore_index=True)\n",
        "\n",
        "TPTN_df.to_csv(path_test+'test_combAll.csv', header=True, index=False)\n",
        "\n",
        "TPTN_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_wB3FQIqOiN"
      },
      "source": [
        "# 8) 可視化\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3v1lf7mRTdS"
      },
      "source": [
        "## 8.1) 重みとバイアスの抽出と計算"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W-w6XhEiYwj7"
      },
      "outputs": [],
      "source": [
        "W1s = []\n",
        "q1 = []\n",
        "s1 = []\n",
        "# q2 = []\n",
        "# s2 = []\n",
        "# q3 = []\n",
        "# s3 = []\n",
        "q_out = []\n",
        "y_pred = []\n",
        "\n",
        "def relu(x):\n",
        "  return x * (x>0)\n",
        "def leakyRELU(x):\n",
        "  return np.where(x > 0, x, x * leakyalpha)\n",
        "# def ELU(x):\n",
        "#     return np.where(x > 0, x, 1.0 * (math.exp(x) - 1.0))\n",
        "def sigomoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "def softmax(x):\n",
        "  if (x.ndim == 1):\n",
        "    x = x[None,:]    # ベクトル形状なら行列形状に変換\n",
        "  # テンソル（x：行列）、軸（axis=1： 列の横方向に計算）\n",
        "  return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "\n",
        "for i in range(5):\n",
        "  W1s.append(np.dot(x_train[i],weight1[i]))\n",
        "  q1.append(W1s[i] + bias1[i])\n",
        "  s1.append(leakyRELU(q1[i]))\n",
        "  q_out.append(np.dot(s1[i], weight_out[i]) + bias_out[i])\n",
        "  y_pred.append(softmax(q_out[i]))\n",
        "  # y_pred = (np.exp(q_out[i]) / (np.sum(np.exp(q_out[i]), axis = 1)).reshape(len(M_all),1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "18vxQWqDmnAx"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "  # print(i+1)\n",
        "  print(y_pred[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SAId4GsK5FHc"
      },
      "outputs": [],
      "source": [
        "#各値の出力\n",
        "for i in range(5):\n",
        "  print('\\n')\n",
        "  print('comb',i+1)\n",
        "  print('\\n')\n",
        "  print(\"x_train\")\n",
        "  print(x_train[i])\n",
        "  # print(\"weight1[:,0]\")\n",
        "  # print(weight1[:,0])\n",
        "  print(\"\\n W1s\")\n",
        "  print(W1s[i])\n",
        "  print(\"\\n bias1\")\n",
        "  print(bias1[i])\n",
        "  print(\"\\n q1\")\n",
        "  print(q1[i])\n",
        "  print(\"\\n s1\")\n",
        "  print(s1[i])\n",
        "  print(\"\\n weight_out\")\n",
        "  print(weight_out[i])\n",
        "  print(\"\\n q_out\")\n",
        "  print(q_out[i])\n",
        "  print(\"\\n y_pred\")\n",
        "  print(y_pred[i])\n",
        "  print(\"\\n bias_out\")\n",
        "  print(bias_out[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_yxBFGeK15i"
      },
      "outputs": [],
      "source": [
        "#計算過程の保存\n",
        "\n",
        "path_process = path_result + 'process/'\n",
        "if os.path.exists(path_process) == 0:\n",
        "  os.mkdir(path_process)\n",
        "\n",
        "for i in range(5):\n",
        "  path_comb = path_process + 'comb' + str(i+1) + '/'\n",
        "  if os.path.exists(path_comb) == 0:\n",
        "    os.mkdir(path_comb)\n",
        "  pro_weight1 = pd.DataFrame(weight1[i])\n",
        "  pro_weight1.to_csv(path_comb+'weight1_comb'+str(i+1)+'.csv', header=True, index=False)\n",
        "  pro_W1s = pd.DataFrame(W1s[i])\n",
        "  pro_W1s.to_csv(path_comb+'W1s_comb'+str(i+1)+'.csv', header=True, index=False)\n",
        "  pro_bias1 = pd.DataFrame(bias1[i])\n",
        "  pro_bias1.to_csv(path_comb+'bias1_comb'+str(i+1)+'.csv', header=True, index=False)\n",
        "  pro_q1 = pd.DataFrame(q1[i])\n",
        "  pro_q1.to_csv(path_comb+'q1_comb'+str(i+1)+'.csv', header=True, index=False)\n",
        "  pro_s1 = pd.DataFrame(s1[i])\n",
        "  pro_s1.to_csv(path_comb+'s1_comb'+str(i+1)+'.csv', header=True, index=False)\n",
        "  pro_weight_out = pd.DataFrame(weight_out[i])\n",
        "  pro_weight_out.to_csv(path_comb+'weight_out_comb'+str(i+1)+'.csv', header=True, index=False)\n",
        "  pro_q_out = pd.DataFrame(q_out[i])\n",
        "  pro_q_out.to_csv(path_comb+'q_out_comb'+str(i+1)+'.csv', header=True, index=False)\n",
        "  pro_y_pred = pd.DataFrame(y_pred[i])\n",
        "  pro_y_pred.to_csv(path_comb+'y_pred_comb'+str(i+1)+'.csv', header=True, index=False)\n",
        "  pro_bias_out = pd.DataFrame(bias_out[i])\n",
        "  pro_bias_out.to_csv(path_comb+'bias_out_comb'+str(i+1)+'.csv', header=True, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFStdiFfK5Vj"
      },
      "outputs": [],
      "source": [
        "#解答の保存\n",
        "\n",
        "path_predict = path_result + 'predict/'\n",
        "if os.path.exists(path_predict) == 0:\n",
        "  os.mkdir(path_predict)\n",
        "\n",
        "for i in range(5):\n",
        "  predict = pd.DataFrame(np.argmax(model_all[i].predict(x_train[i]), axis=-1)+1)\n",
        "  predict.to_csv(path_predict+'predict_comb'+str(i+1)+'.csv', header=True, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3mh9d1kqHQS"
      },
      "source": [
        "## 8.2) 重みの可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt4vhTMnnau5"
      },
      "source": [
        "### 初期の重みの可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YLy728QnnRcp"
      },
      "outputs": [],
      "source": [
        "#初期の重みの可視化\n",
        "path_w1_ini = path_result+'w1_initial/'\n",
        "if os.path.exists(path_w1_ini) == 0:\n",
        "  os.mkdir(path_w1_ini)\n",
        "\n",
        "w_max = np.max(weight1_initial)\n",
        "#w_min = - w_max\n",
        "w_min = np.min(weight1_initial)\n",
        "\n",
        "for i in range(weight1_initial.shape[1]):\n",
        "  print(i+1)\n",
        "  f = weight1_initial[:,i]\n",
        "  f_im = f.reshape((img_h, img_w))\n",
        "  matplotlib.image.imsave(path_w1_ini+'w1_ini_n%d.png'%(i+1),\n",
        "                          f_im, vmin=w_min, vmax=w_max, cmap='jet')\n",
        "  \n",
        "  fig = plt.figure()\n",
        "  im = plt.imshow(f_im,'jet',vmin=w_min,vmax=w_max)\n",
        "  fig.colorbar(im) #orientation='horizontal'\n",
        "\n",
        "  plt.show()\n",
        "#drive/Mydrive/result_???/w1_initialから可視化の様子を見ることが可能"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0L7vcugrHoL"
      },
      "source": [
        "### 学習後の重みの可視化\n",
        "(ここで可視化)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjljl6jnZ95P"
      },
      "outputs": [],
      "source": [
        "#重みの最大値と最小値を出力(この値によって可視化する重みの範囲を決定する)\n",
        "for i in range(5):\n",
        "  print(np.min(weight1[i]))\n",
        "  print(np.max(weight1[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDcxhcuQMP3H"
      },
      "outputs": [],
      "source": [
        "#Visualization of the weight\n",
        "#variables: scale, w_max, w_min\n",
        "#loops: i, f, f_im, im\n",
        "def view_weight1(comb):\n",
        "  k = comb - 1\n",
        "  \n",
        "  scale = 1.0\n",
        "  # if np.max(weight1[k]) < -np.min(weight1[k]):\n",
        "  #   w_max = np.max(weight1[k]) * scale\n",
        "  # else:\n",
        "  #   w_max = (- np.min(weight1[k])) * scale\n",
        "  w_max = 0.03\n",
        "  w_min = - w_max\n",
        "  \n",
        "#   path_w1 = path_result+'w1_trained/'\n",
        "  path_w1 = path_result+'w1_trained/'\n",
        "  if os.path.exists(path_w1) == 0:\n",
        "    os.mkdir(path_w1)\n",
        "\n",
        "  for i in range(weight1[k].shape[1]):\n",
        "    print(i+1)\n",
        "    path_w1_comb = path_w1+'w1_comb'+str(comb)+'/'\n",
        "    \n",
        "    if os.path.exists(path_w1_comb) == 0:\n",
        "      os.mkdir(path_w1_comb)\n",
        "    \n",
        "    f = weight1[k][:,i]\n",
        "    f_im = f.reshape((img_h, img_w))\n",
        "    #f_im = (f_im - w_min)/(w_max - w_min) * 255\n",
        "    matplotlib.image.imsave(path_w1_comb+'w1_comb'+str(comb)+'_n'+str(i+1)+'.png',\n",
        "                            f_im, cmap='jet', vmin=w_min, vmax=w_max)\n",
        "    \n",
        "    fig = plt.figure()\n",
        "    im = plt.imshow(f_im,'jet',vmin=w_min,vmax=w_max)\n",
        "    #im.axes.get_xaxis().set_visible(False)\n",
        "    #im.axes.get_yaxis().set_visible(False)\n",
        "    fig.colorbar(im)\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCJNVCenMR14"
      },
      "outputs": [],
      "source": [
        "for i in range(1,6):\n",
        "  print('comb',i)\n",
        "  view_weight1(i)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "foMZosEGAjTx",
        "VyuUN9rLH1X3"
      ],
      "name": "FNN _付録用",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}